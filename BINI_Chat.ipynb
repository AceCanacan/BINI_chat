{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CY6lOh0z6-So"
      },
      "outputs": [],
      "source": [
        "!pip install pdfkit --quiet\n",
        "!apt-get install -y wkhtmltopdf --quiet\n",
        "!pip install PyPDF2 --quiet\n",
        "!pip install tqdm --quiet\n",
        "!pip install llama-index llama-parse --quiet\n",
        "!pip install llama-index-vector-stores-pinecone --quiet\n",
        "!pip install PyMuPDF --quiet\n",
        "!pip install requests beautifulsoup4 pdfkit  --quiet\n",
        "!pip install llama-index-agent-openai --quiet\n",
        "!pip install llama-index-llms-openai --quiet\n",
        "!pip install llama-index --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbXepJ2zws1o"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8PchFN8dBlH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "import io\n",
        "import pdfkit\n",
        "import fitz\n",
        "from tqdm.notebook import tqdm\n",
        "from bs4 import BeautifulSoup\n",
        "from IPython.display import Markdown, display\n",
        "import random\n",
        "import nest_asyncio\n",
        "from llama_parse import LlamaParse\n",
        "from llama_index.vector_stores.pinecone import PineconeVectorStore\n",
        "from llama_index.core import VectorStoreIndex, StorageContext, Document, SummaryIndex\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.core.node_parser import SimpleNodeParser, SentenceSplitter\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from pinecone import Pinecone, ServerlessSpec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-6nj4iZwt4A"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9wO2247U4bte"
      },
      "outputs": [],
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = \"insert openai api key\"\n",
        "\n",
        "os.environ[\"PINECONE_API_KEY\"] = \"insert pinecone api key\"\n",
        "\n",
        "api_key = os.environ[\"PINECONE_API_KEY\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79tZjHIn57GF"
      },
      "source": [
        "# Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MPvrOtJv2gjU"
      },
      "outputs": [],
      "source": [
        "# List of URLs to scrape\n",
        "url_parts = [\n",
        "    \"8\", \"Aiah\", \"Anagolay_(Born_To_Win)\", \"BINI\", \"BINI/Gallery\",\n",
        "    \"BINI:_The_Launch\", \"BINI_Wiki\", \"BLOOM_(Fandom)\", \"B_HU_U_R\",\n",
        "    \"Born_To_Win\", \"Born_To_Win_(Album)\", \"Born_To_Win_(song)\",\n",
        "    \"Born_To_Win_%3D_Bahasa_Indonesia_Version\", \"Born_To_Win_%3D_Japanese_Version\",\n",
        "    \"Born_To_Win_%3D_Spanish_Version\", \"Born_To_Win_%3D_Thai_Version\",\n",
        "    \"Born_To_Win_Maxi\", \"Colet\", \"Da_Coconut_Nut\", \"Feel_Good_(Album)\",\n",
        "    \"G22\", \"Golden_Arrow\", \"Gwen\", \"Here_With_You\", \"Huwag_Muna_Tayong_Umuwi\",\n",
        "    \"I_Feel_Good\", \"Jhoanna\", \"Kapit_Lang\", \"Karera\", \"Kinikilig\",\n",
        "    \"Lagi\", \"Lian_Kyla\", \"List_of_BINI_Performances\", \"Main_Page\", \"Maloi\",\n",
        "    \"Maloi/Gallery\", \"Mayari_(Born_To_Win)\", \"Mikha\", \"Na_Na_Na\", \"No_Fear\",\n",
        "    \"One_Dream:_The_BINI-BGYO_Journey\", \"Pantropiko\", \"SAB\", \"Sheena\",\n",
        "    \"Sodop_(Born_To_Win)\", \"Stacey\", \"Star_Hunt_Academy\", \"Star_Hunt_Trainee_TV\",\n",
        "    \"Strings\", \"Strings_-_Dance_ver.\", \"Zero_World_(Born_To_Win)\"\n",
        "]\n",
        "\n",
        "# Function to generate full URLs\n",
        "def generate_urls(base_url, parts):\n",
        "    return [f\"{base_url}{part}\" for part in parts]\n",
        "\n",
        "# Base URL\n",
        "base_url = \"https://bini.fandom.com/wiki/\"\n",
        "\n",
        "# Generate full URLs\n",
        "urls = generate_urls(base_url, url_parts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gj9OtOUsF6_S"
      },
      "outputs": [],
      "source": [
        "# Create a directory to save individual text files\n",
        "os.makedirs(\"text_data\", exist_ok=True)\n",
        "\n",
        "# Function to extract text from a URL using BeautifulSoup\n",
        "def extract_text_from_url(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Find the main content area\n",
        "    content_div = soup.find('div', {'class': 'mw-parser-output'})\n",
        "    if content_div:\n",
        "        text = content_div.get_text(separator='\\n', strip=True)\n",
        "        return text\n",
        "    return \"\"\n",
        "\n",
        "# Scrape text from each URL and save it to a text file, with a progress bar\n",
        "text_files = []\n",
        "for i, url in enumerate(tqdm(urls, desc=\"Scraping URLs for text\")):\n",
        "    text_filename = f\"text_data/page_{i+1}.txt\"\n",
        "    text = extract_text_from_url(url)\n",
        "    with open(text_filename, 'w', encoding='utf-8') as file:\n",
        "        file.write(text)\n",
        "    text_files.append(text_filename)\n",
        "\n",
        "# Convert each text file to a PDF\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "pdf_files = []\n",
        "for i, text_file in enumerate(tqdm(text_files, desc=\"Converting text to PDFs\")):\n",
        "    pdf_filename = f\"data/page_{i+1}.pdf\"\n",
        "    pdfkit.from_file(text_file, pdf_filename)\n",
        "    pdf_files.append(pdf_filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5dn5-UFLKVp"
      },
      "source": [
        "# Loading with LlamaParse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42OIzaBALPKC"
      },
      "outputs": [],
      "source": [
        "# Apply nest_asyncio to allow nested event loops (useful in Jupyter/Colab notebooks)\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Initialize LlamaParse with necessary parameters\n",
        "parser = LlamaParse(\n",
        "    api_key=llama_parse_key,  # Replace with your actual API key\n",
        "    result_type=\"text\",  # \"markdown\" and \"text\" are available\n",
        "    verbose=False,  # Set to False to reduce verbosity\n",
        "    language=\"en\",  # Optionally you can define a language, default=en\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vmRrzn6_xFUO"
      },
      "outputs": [],
      "source": [
        "# Define the directory containing the PDF files\n",
        "data_directory = \"/content/data\"\n",
        "\n",
        "# List all PDF files in the directory\n",
        "document_paths = [os.path.join(data_directory, file) for file in os.listdir(data_directory) if file.endswith(\".pdf\")]\n",
        "\n",
        "# Initialize an empty list to store the processed documents\n",
        "processed_documents = []\n",
        "\n",
        "# Process each document with a single progress bar\n",
        "for document_path in tqdm(document_paths, desc=\"Processing documents\"):\n",
        "    document = parser.load_data([document_path])\n",
        "    processed_documents.extend(document)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GrR_-h9Ue6mL"
      },
      "outputs": [],
      "source": [
        "# Display the total number of documents\n",
        "total_documents = len(processed_documents)\n",
        "print(f\"Total number of documents: {total_documents}\")\n",
        "\n",
        "# Randomly select one document to print\n",
        "random_index = random.randint(0, total_documents - 1)\n",
        "print(f\"Randomly selected document {random_index + 1}:\\n\")\n",
        "display(processed_documents[random_index])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eh1_fV_7T2uT"
      },
      "source": [
        "# Preparing Storing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umjyDLqdlBhP"
      },
      "outputs": [],
      "source": [
        "node_parser = SimpleNodeParser()\n",
        "\n",
        "nodes = node_parser.get_nodes_from_documents(processed_documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3wfrcoiv8DV"
      },
      "outputs": [],
      "source": [
        "# Print the number of nodes parsed\n",
        "print(f\"Number of nodes parsed: {len(nodes)}\")\n",
        "\n",
        "# Print details of the first few nodes to inspect their structure\n",
        "for i, node in enumerate(nodes[:3]):  # Adjust the range if needed\n",
        "    print(f\"\\nNode {i} details:\")\n",
        "    display(node)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7t6ddgFET6lj"
      },
      "outputs": [],
      "source": [
        "# Function to create an index if it doesn't exist\n",
        "def create_index_if_not_exists(index_name, dimension, metric, spec):\n",
        "    existing_indexes = pc.list_indexes()\n",
        "    if index_name not in existing_indexes:\n",
        "        pc.create_index(\n",
        "            name=index_name,\n",
        "            dimension=dimension,\n",
        "            metric=metric,\n",
        "            spec=spec\n",
        "        )\n",
        "        print(f\"Index '{index_name}' created successfully.\")\n",
        "    else:\n",
        "        print(f\"Index '{index_name}' already exists.\")\n",
        "\n",
        "# Usage\n",
        "create_index_if_not_exists(\n",
        "    index_name=\"bini_rag\",\n",
        "    dimension=1536,\n",
        "    metric=\"dotproduct\",\n",
        "    spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mMg78IH6AsZ"
      },
      "outputs": [],
      "source": [
        "pinecone_index = pc.Index(\"bini_rag\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSBQ8--857HZ"
      },
      "outputs": [],
      "source": [
        "vector_store = PineconeVectorStore(\n",
        "    pinecone_index=pinecone_index,\n",
        "    add_sparse_vector=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-UmL-nTUjaL"
      },
      "source": [
        "# Indexing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qH5nB2qxUnAn"
      },
      "outputs": [],
      "source": [
        "# Specify your OpenAI API key and the embedding model you want to use\n",
        "embedding_model = \"text-embedding-3-small\"  # Replace with the actual model name\n",
        "\n",
        "# Initialize the OpenAIEmbedding with the specified model\n",
        "embed_model = OpenAIEmbedding(api_key=api_key, model=embedding_model)\n",
        "\n",
        "# Assume astra_db_store is already initialized\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03n6RRKlRaTF"
      },
      "outputs": [],
      "source": [
        "# Initialize the VectorStoreIndex with the nodes and storage context\n",
        "index = VectorStoreIndex(\n",
        "    nodes=nodes,\n",
        "    storage_context=storage_context,\n",
        "    embed_model=embed_model,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SAFJx_XYR8FY"
      },
      "outputs": [],
      "source": [
        "# Create the VectorStoreIndex using the existing Pinecone collection\n",
        "index = VectorStoreIndex.from_vector_store(\n",
        "    vector_store,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxWSNet00fIY"
      },
      "source": [
        "# Querying"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tnU6lIaowZ28"
      },
      "outputs": [],
      "source": [
        "# Set up the query engine\n",
        "query_engine = index.as_query_engine(vector_store_query_mode=\"hybrid\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JqH4NUYG48gb"
      },
      "outputs": [],
      "source": [
        "# Example query\n",
        "response = query_engine.query(\"What is Colet's full name?\")\n",
        "print(response)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
